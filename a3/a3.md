# CS340 - Assignment 3
Tristan Rice - q7w9a - 25886145

# 1. Vectors, Matrices, and Quadratic Functions

## 1.1. Basic Operations

### 1.1.1.
$$x^Tx = 2*2 + 3*3 = 13$$

### 1.1.2.
$$\lVert x \rVert^2 = x^Tx = 13$$

### 1.1.3.
$$x^T(x+\alpha y) =
\begin{bmatrix}
2 & 3
\end{bmatrix}
\begin{bmatrix}
2 + 5*1 \\
3 + 5*4
\end{bmatrix}
= 2 *(2+5*1) + 3(3+5*4) = 83
$$

### 1.1.4.
$$
Ax = \begin{bmatrix}
2*1 + 3*2 \\
2*2 + 3*3 \\
3*3 + 3*2
\end{bmatrix}
=
\begin{bmatrix}
8 \\
13 \\
15
\end{bmatrix}
$$

### 1.1.5.
$$
z^TAx=
\begin{bmatrix}
2 & 0 & 1
\end{bmatrix}
Ax
= 2*8 + 0 * 13 + 1*15 = 31
$$

### 1.1.6.
$$A^TA =
\begin{bmatrix}
1 & 2 & 3\\
2 & 3 & 2
\end{bmatrix}
\begin{bmatrix}
1 & 2\\
2 & 3\\
3 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 *1 + 2*2 + 3*3 & 1 * 2 + 2 * 3 + 3 * 2 \\
1 * 2 + 2 * 3 + 3 * 2 & 2*2 + 3*3+2*2
\end{bmatrix}
=
\begin{bmatrix}
14 & 14 \\
14 & 17
\end{bmatrix}
$$

### 1.1.7. $yy^Ty=\lVert y\rVert^2y$

True. Matrix multiplication is associative. $y(y^Ty)=(y^Ty)y$. $y^Ty$ produces a
scalar, and scalar-matrix multiplication is commutative.

#### Alternate Way

$$
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
\begin{bmatrix}
y_1& y_2& y_3
\end{bmatrix}
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
=
\begin{bmatrix}
y_1& y_2& y_3
\end{bmatrix}
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
$$

$$
\begin{bmatrix}
y_1^2 & y_1*y_2 & y_1*y_3 \\
y_1 * y_2 & y_2^2 & y_2*y_3 \\
y_1 * y_3 & y_2*y_3 & y_3^2
\end{bmatrix}
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
=
(y_1^2+y_2^2+y_3^2)
\begin{bmatrix}
y_1\\
y_2\\
y_3
\end{bmatrix}
$$
$$
\begin{bmatrix}
y_1^3+y_1*y_2^2+y_1*y_3^2) \\
y_1^2*y_2+y_2^3+y_2*y_3^2) \\
y_1^2*y_3+y_2^2*y_3+y_3^3)
\end{bmatrix}
=
\begin{bmatrix}
y_1^3+y_1*y_2^2+y_1*y_3^2) \\
y_1^2*y_2+y_2^3+y_2*y_3^2) \\
y_1^2*y_3+y_2^2*y_3+y_3^3)
\end{bmatrix}
$$

### 1.1.8. $x^TA^T(Ay+Az)=x^TA^TAy+z^TA^TAx$

True. Multiplication is distributive.
$$x^TA^TAy + x^TA^TAz = x^TA^TAy+z^TA^TAx$$
$A^TA$ produces a scalar which is communative and $z^Tx = $x^Tz$.

### 1.1.9. $A^T(B+C)=BA^T+CA^T$
False. Matrix multiplication is distributive, but not communative.
$$A^TB + A^TC\neq BA^T+CA^T$$

### 1.1.10. $x^T(B+C) = Bx + Cx$
???
$$x^TB + x^TC = Bx + Cx$$

### 1.1.11. $(A+BC)^T = A^T+C^TB^T$

True.

$$A^T+(BC)^T = A^T+C^T$$
$$A^T+C^TB^T = A^T+C^TB^T$$

### 1.1.12. $(x-y)^T(x-y)=\lVert x \rVert^2-x^Ty+\lVert y\rVert^2$

False.

$$(x^T-y^T)(x-y)$$
$$(x^T-y^T)x-(x^T-y^T)y)$$
$$x^Tx-y^Tx-x^Ty+y^Ty)$$
$$\lVert x \rVert^2-2x^Ty+\lVert y \rVert^2$$

### 1.1.13. $(x-y)^T(x+y) = \lVert x\rVert^2-\lVert y\rVert^2$

True.

$$(x^T-y^T)(x+y)$$
$$(x^T-y^T)x+(x^T-y^T)y)$$
$$x^Tx-y^Tx+x^Ty-y^Ty)$$
$$\lVert x \rVert^2-\lVert y \rVert^2$$

## 1.2

### 1.2.1. $\sum_{n=1}^n |w^Tx_i-y_i| = w^TX - y$

$$[1, 1, \ldots, 1](w^TX-y)$$

### 1.2.2. $\max_{i\in\{1,2,\ldots,n\}} |w^Tx_i-y_i|+\frac\lambda2\sum_{j=1}^nw_j^2$

$$\log([1, 1, \ldots, 1]\exp(w^TX-y))+\frac\lambda2 \lVert w \rVert^2$$

exp is component wise exponentiation.

### 1.2.3. $\sum_{i=1}^n z_i(w^Tx_i-y_i)^2+\lambda\sum_{j=1}^d|w_j|$

$$ (Xw-y)^TZ(Xw-y) + \lambda \lVert w\rVert $$

## 1.3

### 1.3.1. $f(w)=\frac12 \lVert w-v\rVert^2$

$$\frac{d f(w)}{d w_1} = \frac12 \frac{d}{d w_1} ((w_1-v_1)^2 + (w_2-v_2)^2 + \ldots)$$
$$\frac{d f(w)}{d w_1} = \frac12 2(w_1-v_1)$$
$$\frac{d f(w)}{d w_1} = w_1-v_1$$
$$\nabla f(w) = [\frac{d f(w)}{d w_1}, \ldots, \frac{d f(w)}{d w_n}]$$

$$\nabla f(w) = w-v = 0$$
$$w = v$$

### 1.3.2 $f(w)=\frac12\lVert w \rVert^2+w^TX^Ty$
$$f(w)=\frac12\lVert w \rVert^2+y^TX^Tw$$
$$\nabla f(w) = w + y^TX^T = 0$$
$$w = -y^TX^T$$

### 1.3.3. $f(w) = \frac12\lVert Xw-y\rVert^2 + \frac12 w^T \Lambda w$

$$\nabla f(w) = X(Xw-y) + [1, \ldots, 1]\Lambda w) = 0$$
$$XXw-Xy + [1, \ldots, 1]\Lambda w) = 0$$
$$(XX + [1, \ldots, 1]\Lambda)w = Xy$$
$$w = (XX + [1, \ldots, 1]\Lambda)^{-1} Xy$$

### 1.3.4 $f(w)=\frac12\sum_{i=1}^n z_i(w^Tx_i-y_i)^2$

$$f(x) = \frac12 (Xw-y)^TZ(Xw-y)$$
$$f(x) = \frac12 (w^TX^T-y^T)Z(Xw-y)$$
$$f(x) = \frac12 (w^TX^T-y^T)(ZXw-Zy)$$
$$f(x) = \frac12 ((w^TX^T-y^T)ZXw-(w^TX^T-y^T)Zy)$$
$$f(x) = \frac12 (w^TX^TZXw - y^TZXw - w^TX^TZy + y^TZy)$$

$$\nabla f(x) = X^TZXw - \frac12 y^TZX - \frac12 X^TZy = 0$$
$$ X^TZXw = \frac12 y^TZX + \frac12 X^TZy $$
$$ w = \frac12 ( (X^TZX)^{-1}y^TZX + (X)^{-1}y)$$

#### Bad
$$\nabla f(x) = X^TZXw - X^TZy = 0$$
$$X^TZXw = X^TZy$$
$$w = (X^TZX)^{-1}(X^TZy)$$
$$w = (X^TZX)^{-1}(X^TZy)$$


#### Really bad

$$f(w) = \frac12 \sum_{i=1}^n z_i((w^Tx_i-y_i)w^Tx_i-(w^Tx_i-y_i)y_i)$$
$$f(w) = \frac12 \sum_{i=1}^n z_i(w^Tx_iw^Tx_i-y_iw^Tx_i-w^Tx_iy_i+y_iy_i)$$

$$\nabla f(w)=\sum_{i=1}^n z_i x_i (w^Tx_i-y_i) = 0$$
$$\sum_{i=1}^n z_i x_i w^Tx_i-z_i x_i y_i = 0$$
$$\sum_{i=1}^n z_i x_i w^Tx_i = \sum_{i=1}^n z_i x_i y_i$$
$$\sum_{i=1}^n w^Tx_i = \sum_{i=1}^n y_i$$
$$w = ((\sum_{i=1}^n y_i)x^{-1})^T$$

# 2. Linear Regression and Nonlinear Bases
# 2.1. Adding a Bias Variable

```octave
function [model] = leastSquaresBias(X,y)

X = [ones(rows(X), 1) X];

% Solve least squares problem
w = (X'*X)\X'*y;

model.w = w;
model.predict = @predict;

end

function [yhat] = predict(model,Xhat)
Xhat = [ones(rows(Xhat), 1) Xhat];
w = model.w;
yhat = Xhat*w;
end
```

![](./2.1.png)\

```
Training error = 3551.35
Test error = 3393.87
```

# 2.2. Polynomial Basis

```octave
function [model] = leastSquaresBasis(X,y,p)

X = polyBasis(X, p);

% Solve least squares problem
% We're using inv, since octave works better when dealing with large matrixes.
w = inv(X'*X)*X'*y;

model.w = w;
model.p = p;
model.predict = @predict;

end

function [yhat] = predict(model,Xhat)
Xhat = polyBasis(Xhat, model.p);
w = model.w;
yhat = Xhat*w;
end

function [Xpoly] = polyBasis(X, p)
Xpoly = [];
for i = 0:p
  Xpoly = [Xpoly X.^i];
end
end
```

```
p = 0:
Training error = 15480.52
Test error = 14390.76
p = 1:
Training error = 3551.35
Test error = 3393.87
p = 2:
Training error = 2167.99
Test error = 2480.73
p = 3:
Training error = 252.05
Test error = 242.80
p = 4:
Training error = 251.46
Test error = 242.13
p = 5:
Training error = 251.14
Test error = 239.54
p = 6:
Training error = 248.58
Test error = 246.01
p = 7:
Training error = 247.01
Test error = 242.89
p = 8:
Training error = 241.31
Test error = 245.97
p = 9:
Training error = 235.76
Test error = 259.30
p = 10:
Training error = 235.07
Test error = 256.30
```

Increasing $p$ lowers the training error, but doesn't necessarily lower the test
error. With very high p values, the model becomes over fit and performs less
well.

# 2.3. Manual Search for Optimal Basis

Fitting a $p=5$ Least Squares Basis and then adding $20*\sin(X*5)$ gives us

```
Training error = 50.99
Test error = 53.71
```

![](./2.3.png)\

This is a pretty good fit. However, from the naked eye, it looks like it could be a bit
better.

I obtained this basis from using the best $p$ value from the previous question,
and then manually measuring the number of peaks. Manually counting the number of
peaks and then estimating the height of them gives a pretty good estimate for
the coefficients for the sin term.

# 3. Non-Parametric Bases and Cross-Validation

## 3.1. Proper Training and Validation Sets

The issue is that the training and validation sets aren't both representative of
the whole dataset since they're respectively the first and second half of it. If
you instead randomize the selection it works much better and is a better
representation of the dataset.

```octave
ordering = randperm(n);
X = X(ordering,:);
y = y(ordering,:);
Xtrain = X(1:n/2,:);
ytrain = y(1:n/2,:);
Xvalid = X(n/2+1:end,:);
yvalid = y(n/2+1:end,:);
```

```
Training error = 39.49
Test error = 71.17
```

## 3.2. Cross-Validation

```octave
% Load data
load basisData.mat % Loads X and y
[n,d] = size(X);

% Split training data into a training and a validation set
ordering = randperm(n);
X = X(ordering,:);
y = y(ordering,:);

valsize = n/10;

bestSigmas = [];

for cv=1:10
    validation = ((cv-1)*valsize+1):((cv)*valsize);
    training = setdiff(1:n, validation);
    Xtrain = X(training,:);
    ytrain = y(training,:);
    Xvalid = X(validation,:);
    yvalid = y(validation,:);

    % Find best value of RBF kernel parameter,
    %   training on the train set and validating on the validation set
    minErr = inf;
    for sigma = 2.^[-15:15]

        % Train on the training set
        model = leastSquaresRBF(Xtrain,ytrain,sigma);

        % Compute the error on the validation set
        yhat = model.predict(model,Xvalid);
        validError = sum((yhat - yvalid).^2)/(n/2);
        fprintf('Error with sigma = %.3e = %.2f\n',sigma,validError);

        % Keep track of the lowest validation error
        if validError < minErr
            minErr = validError;
            bestSigma = sigma;
        end
    end
    fprintf('Value of sigma that achieved the lowest validation error was %.3e\n',bestSigma);
    bestSigmas = [bestSigmas bestSigma];
end

bestSigma = mean(bestSigmas)

% Now fit the model based on the full dataset.
fprintf('Refitting on full training set...\n');
model = leastSquaresRBF(X,y,bestSigma);

% Compute training error
yhat = model.predict(model,X);
trainError = sum((yhat - y).^2)/n;
fprintf('Training error = %.2f\n',trainError);

% Finally, report the error on the test set
t = size(Xtest,1);
yhat = model.predict(model,Xtest);
testError = sum((yhat - ytest).^2)/t;
fprintf('Test error = %.2f\n',testError);
```

Typical output

```
bestSigma =  0.95000
Refitting on full training set...
Training error = 39.16
Test error = 63.29
```

The best $\sigma$ value is typically 0.95, but is sometimes 1 or 0.9.


## 3.3. Cost of Non-Parametric Bases

TODO

## 3.4. Non-Parametric Bases with Uneven Data

When the period of oscillations isn't constant, it's much harder to fit an RBFs
to it since the optimal n will change. For high frequency sections, the RBF needs
to be much closer together. When there are low frequency sections, the RBFs may
be denser than required.

If we haven't evenly sampled the training data, there may be gaps or sections at
either end of the domain that the RBFs don't have data for.

To fix these issues, you could add bias and linear basis so it acts like linear
regression instead of zero when there isn't data close to that point.

# 4. Robust Regression and Gradient Descent

## 4.1. Weighted Least Squares in One Dimension

![](./4.1.png)\

TODO

