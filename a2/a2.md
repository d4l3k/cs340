# 1 K-Nearest Neighbours
## 1.1 KNN Prediction

### 1.1.1 Handin your predict function.

```matlab
function [yhat] = predict(model,Xtest)
  % compute euclidian distance
  % distances = sum((model.X - Xtest).**2, 2);
  [n,d] = size(model.X);
  [t,d] = size(Xtest);

  distances = model.X.^2*ones(d,t) + ones(n,d)*(Xtest').^2 - 2*model.X*Xtest';
  yhat = zeros(t, 1);
  for c = 1:t
    [dists,  idx] = sort(distances(:, c));
    yhat(c) = mode(model.y(idx(1:model.k, :)));
  end
end
```

### 1.1.2 Report the training and test error obtained on the citiesSmall.mat dataset for k = 1, k = 3, and k = 10.

```
Training error with k-1 knn: 0.000
Test error with k-1 knn: 0.065
Training error with k-3 knn: 0.028
Test error with k-3 knn: 0.066
Training error with k-10 knn: 0.072
Test error with k-10 knn: 0.097
```

### 1.1.3 Hand in the plot generatied by classifier2Dplot on the citiesSmall.mat dataset for k = 1. (Note that this version of the function also plots the test data.)

![](./1.1.3.png)\

### 1.1.4 Why is the training error 0 for k = 1?

Since all the training data forms the model, when k = 1 the training data
matches exactly the test data thus the distance is zero. It finds the exact
matching point in the training data.

### 1.1.5 If you didn’t have an explicit test set, how would you choose k?

You could split the training set in half and use one half for training and one
for testing. Alternatively you could make an educated guess for k.

## 1.2 Condensed Nearest Neighbours

### 1.2.1. Hand in your cnn.m code.

```matlab
function [model] = cnn(X,y,k)
  % [model] = cnn(X,y,k)
  %
  % Implementation of condensed-nearest neighbour classifer

  % Always add the first point as a start.
  model.X = X(1, :);
  model.y = y(1, :);
  model.k = k;
  model.c = max(y);
  model.predict = @predict;

  [n, d] = size(X);
  for i = 2:n
    if model.predict(model, X(i, :)) != y(i)
      model.X = [model.X; X(i, :)];
      model.y = [model.y; y(i)];
    end
  end
end

function [yhat] = predict(model,Xtest)
  % compute euclidian distance
  % distances = sum((model.X - Xtest).**2, 2);
  [n,d] = size(model.X);
  [t,d] = size(Xtest);

  distances = model.X.^2*ones(d,t) + ones(n,d)*(Xtest').^2 - 2*model.X*Xtest';
  yhat = zeros(t, 1);
  for c = 1:t
    [dists,  idx] = sort(distances(:, c));
    yhat(c) = mode(model.y(idx(1:min(n, model.k), :)));
  end
end
```

### 1.2.2. Report the training and testing errors, as well as the number of objects in the subset, on the citiesBig1.mat dataset with k = 1.

```
Number of objects in model 457 of 14735
Training error with k-1 cnn: 0.008
Test error with k-1 cnn: 0.018
```

### 1.2.3. Hand in the plot generated by classifier2Dplot on the citiesBig1.mat dataset for k = 1

![](./1.2.3.png)\

### 1.2.4. Why is the training error with k = 1 now greater than 0?

It's now greater than zero because not all of the provided training points are
included in the model. Only those that improve the classifier score.

### 1.2.5. If you have s examples in the subset, what is the cost of running the predict function on t test examples in terms of n, d, t, and s?

It comes out to be $t * d * s$. Doesn't depend on n since s is a subset of n.

### 1.2.6. Try out your function on the dataset in citiesBig2.mat. Why are the test error and training error so high (even for k = 1) for this method on this dataset?

```
Training error with k-1 knn: 0.138
Test error with k-1 knn: 0.210
```

Probably because there's a lot of datapoints near the edge of regions. It has
lots of variation so is hard to predict.

# 2. Random Forests

## 2.1. Random Trees

### 2.1.1. Make a plot of the training error and the test error for the decisionTree model, as the depth is varied from 1 to 15.

![](./2.1.1.png)\

### 2.1.2. Why does the decisionTree function terminate if you set the depth parameter to ∞?

It terminates because every variable has been fit, and the model is so overfit
it exactly matches the training data. When there are no more possible rules that
increase information gain, the algorithm stops.

### 2.1.3. Copy the decisionStump function to a new function called randomStump. Modify the randomStump so that it only considers b √ dc randomly-chosen features. Hand in the new training function and make a plot of the training and test error of the now-working randomTree model as the depth is varied from 1 to 15 with this method (it should not be the same every time, so just include one run of the method).

```matlab
function [model] = randomStump(X,y)
% [model] = randomStump(X,y)
%
% Fits a decision stump that splits on a single variable.

% Compute number of training examples and number of features
[n,d] = size(X);

% Computer number of class lables
k = max(y);

% Address the trivial case where we do not split
count = accumarray(y,ones(size(y)),[k 1]); % Counts the number of occurrences of each class
[maxCount,maxLabel] = max(count);

% Compute total entropy (needed for information gain)
p = count/sum(count); % Convert counts to probabilities
entropyTotal = -sum(p.*log0(p));

maxGain = 0;
splitVariable = [];
splitThreshold = [];
splitLabel0 = maxLabel;
splitLabel1 = [];

% Loop over sqrt(d) random features looking for the best split
if any(y ~= y(1))
    for j = floor(rand(1,sqrt(d))*d+1)
        thresholds = sort(unique(X(:,j)));

        for t = thresholds'

            % Count number of class labels where the feature is greater than threshold
            yVals = y(X(:,j) > t);
            count1 = accumarray(yVals,ones(size(yVals)),[k 1]);
            count0 = count-count1;

            % Compute infogain
            p1 = count1/sum(count1);
            p0 = count0/sum(count0);
            H1 = -sum(p1.*log0(p1));
            H0 = -sum(p0.*log0(p0));
            prob1 = sum(X(:,j) > t)/n;
            prob0 = 1-prob1;
            infoGain = entropyTotal - prob1*H1 - prob0*H0;

            % Compare to minimum error so far
            if infoGain > maxGain
                % This is the lowest error, store this value
                maxGain = infoGain;
                splitVariable = j;
                splitThreshold = t;

                % Compute majority class
                [maxCount,splitLabel1] = max(count1);
                [maxCount,splitLabel0] = max(count0);
            end
        end
    end
end
model.splitVariable = splitVariable;
model.splitThreshold = splitThreshold;
model.label1 = splitLabel1;
model.label0 = splitLabel0;
model.predict = @predict;
end
```

![](./2.1.3.png)\

### 2.1.4. Make a third training/test plot where you use the decisionTree model but for each depth you train on a different boostrap sample of the training data (but evaluate the training error on the original training data).


